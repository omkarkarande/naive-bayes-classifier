################################################################################
README - INSTRUCTIONS ON RUNNING EACH SCRIPT
AUTHOR: OMKAR KARANDE
USC-EMAIL: karande@usc.edu
################################################################################

FORMAT OF THE FILES:
    -Each file is in the Project data format viz. LABEL FEATURE1:VALUE1 FEATURE2:VALUE2 ...
    -FEATURE identifiers start from 1 for all files.


The included scripts and their descriptions are as follows:


1.  nblearn.py:
    -This is the python script to call the Naive Bayes classifier in learning mode.
    -USAGE: python3 nblearn.py <Training File> <Model File>
    -Takes in a training file in the Project Data Format and generates a model file with the name specified.

2.  nbclassify.py:
    -This is the python script to call the Naive Bayes classifier in classification/prediction mode.
    -USAGE: python3 nbclassify.py <Model File Generated by nblearn.py> <Testing File>
    -NOTE: Testing data must be in Project data format and must NOT contain any labels. Each line should only consist of features and their respective values.
    -Dumps the predictions to stdout. use '>' to write into a file.

3.  NB.py:
    -This is my python implementation of the Naive Bayes Classifier.
    -Generic implementation that is used by both nblearn.py and nbclassify.py
    -IMPORTANT: REQUIRED TO RUN nblearn and nbclassify

4.  data_partitioner.py:
    -This python script randomizes the data in the file passed to it as an argument and generate splits based on the percentage provided
    -Usage: python3 data_partitioner.py <File> <Trainingset percentage> <Linear split or Random>
            File: Path to the file you wish to split
            Percentage: integer value between 1 to 100 - Determines the size of the Training Set
            Linear/Random: 1 for linear split and 0 for random split

5.  analysis.py:
    -Calculates the statistics of the prediction result obtained.
    -USAGE: python3 analysis.py <Actual Labels> <Predicted Labels>
    -NOTE: Files should contain labels in their text forms either POSITIVE, NEGATIVE, SPAM or HAM. Each line should contain the prediction for the record at that line.

6.  preprocess_delabel.py:
    -Removes labels from labeled files in PDF format
    -USAGE: python3 preprocess_delabel.py <input file> <output file>

7.  preprocess_imdb.py:
    -Preprocesses the IMDB data
    -Converts numeric labels >=7 to 'POSITIVE' and <=4 to 'NEGATIVE'
    -Increments the feature identifier by 1 for all features in the given file
    -USAGE:  python3 preprocess_imdb.py INPUT OUTPUT LABELED/UNLABELED[1/0]
            Labeled/Unlabeled   = 1 for files containing labels
                                = 0 for files not containing labels (feature ID increment only)

8.  preprocess_email.py:
    -Preprocesses the Email dataset
    -Reads emails from all files from the given directory and compiles them into a single dataset
    -Orders the file according to filename for Testing Data only
    -USAGE: python3 preprocess_email.py INPUT OUTPUT VOCAB LABELED/UNLABELED[1/0]
            INPUT - Directory with all the emails in
            OUTPUT - File name for the compiled corpus
            VOCAB - Path to the vocabulary file
            LABELED/UNLABELED   = 1 for LABELED DATA like the Training data where order is not important and labels exist
                                = 0 for UN-Labeled data like the Testing data where order is important and labels dont exist

9.  svm_preprocess.py:
    -Preprocesses data to be compatible with SVMlight
    -USAGE: python3 svm_preprocess.py INPUT OUTPUT INSERT_DUMMY_LABEL[1/0]
    -Input and Output denote the corresponding files
    -INSERT DUMMY LABEL = 1 when you want the processor to insert a dummy label for generating a file used to classification in SVMLight

10. svm_postprocess.py:
    -Converts output generated by SVMLight to labels used in this project (POSITIVE, NEGATIVE, SPAM, HAM)
    -USAGE: python3 svm_postprocess.py INPUT OUTPUT TYPE
    -TYPE = 'imdb' for sentiment dataset
            'enron' for spam/ham dataset

11. megam_preprocess.py:
    -Preprocesses data to be compatible with MegaM
    -USAGE: python3 megam_preprocess.py INPUT OUTPUT INSERT_DUMMY_LABEL[1/0]
    -Input and Output denote the corresponding files
    -INSERT DUMMY LABEL = 1 when you want the processor to insert a dummy label for generating a file used to classification in MegaM

12. megam_postprocess.py:
    -Converts output generated by MegaM to labels used in this project (POSITIVE, NEGATIVE, SPAM, HAM)
    -USAGE: python3 megam_postprocess.py INPUT OUTPUT TYPE
    -TYPE = 'imdb' for sentiment dataset
            'enron' for spam/ham dataset


################################################################################
GENERAL USAGE GUIDELINES
################################################################################

1.  Always Preprocess all data. The code uses feature identifier values starting from 1 for all classifiers. Preprocessing ensures the data is in
    the format needed by all the codes.
2.  Alwasy use the preprocessed data for any of the following tasks like svm_preprocess, de-labeling, etc. to make sure all scripts stay compatible.


################################################################################
ANALYSIS AND INFERENCES
################################################################################
