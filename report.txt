################################################################################
README - INSTRUCTIONS ON RUNNING EACH SCRIPT
AUTHOR: OMKAR KARANDE
USC-EMAIL: karande@usc.edu
################################################################################

FORMAT OF THE FILES:
    -Each file is in the Project data format viz. LABEL FEATURE1:VALUE1 FEATURE2:VALUE2 ...
    -FEATURE identifiers start from 1 for all files.


The included scripts and their descriptions are as follows:


1.  nblearn.py:
    -This is the python script to call the Naive Bayes classifier in learning mode.
    -USAGE: python3 nblearn.py <Training File> <Model File>
    -Takes in a training file in the Project Data Format and generates a model file with the name specified.

2.  nbclassify.py:
    -This is the python script to call the Naive Bayes classifier in classification/prediction mode.
    -USAGE: python3 nbclassify.py <Model File Generated by nblearn.py> <Testing File>
    -NOTE: Testing data must be in Project data format and must NOT contain any labels. Each line should only consist of features and their respective values.
    -Dumps the predictions to stdout. use '>' to write into a file.

3.  NB.py:
    -This is my python implementation of the Naive Bayes Classifier.
    -Generic implementation that is used by both nblearn.py and nbclassify.py
    -IMPORTANT: REQUIRED TO RUN nblearn and nbclassify

4.  data_partitioner.py:
    -This python script randomizes the data in the file passed to it as an argument and generate splits based on the percentage provided
    -Usage: python3 data_partitioner.py <File> <Trainingset percentage> <Linear split or Random>
            File: Path to the file you wish to split
            Percentage: integer value between 1 to 100 - Determines the size of the Training Set
            Linear/Random: 1 for linear split and 0 for random split

5.  analysis.py:
    -Calculates the statistics of the prediction result obtained.
    -USAGE: python3 analysis.py <Actual Labels> <Predicted Labels>
    -NOTE: Files should contain labels in their text forms either POSITIVE, NEGATIVE, SPAM or HAM. Each line should contain the prediction for the record at that line.

6.  preprocess_delabel.py:
    -Removes labels from labeled files in PDF format
    -USAGE: python3 preprocess_delabel.py <input file> <output file>

7.  preprocess_imdb.py:
    -Preprocesses the IMDB data
    -Converts numeric labels >=7 to 'POSITIVE' and <=4 to 'NEGATIVE'
    -Increments the feature identifier by 1 for all features in the given file
    -USAGE:  python3 preprocess_imdb.py INPUT OUTPUT LABELED/UNLABELED[1/0]
            Labeled/Unlabeled   = 1 for files containing labels
                                = 0 for files not containing labels (feature ID increment only)

8.  preprocess_email.py:
    -Preprocesses the Email dataset
    -Reads emails from all files from the given directory and compiles them into a single dataset
    -Orders the file according to filename for Testing Data only
    -USAGE: python3 preprocess_email.py INPUT OUTPUT VOCAB LABELED/UNLABELED[1/0]
            INPUT - Directory with all the emails in
            OUTPUT - File name for the compiled corpus
            VOCAB - Path to the vocabulary file
            LABELED/UNLABELED   = 1 for LABELED DATA like the Training data where order is not important and labels exist
                                = 0 for UN-Labeled data like the Testing data where order is important and labels dont exist

9.  svm_preprocess.py:
    -Preprocesses data to be compatible with SVMlight
    -USAGE: python3 svm_preprocess.py INPUT OUTPUT INSERT_DUMMY_LABEL[1/0]
    -Input and Output denote the corresponding files
    -INSERT DUMMY LABEL = 1 when you want the processor to insert a dummy label for generating a file used to classification in SVMLight

10. svm_postprocess.py:
    -Converts output generated by SVMLight to labels used in this project (POSITIVE, NEGATIVE, SPAM, HAM)
    -USAGE: python3 svm_postprocess.py INPUT OUTPUT TYPE
    -TYPE = 'imdb' for sentiment dataset
            'enron' for spam/ham dataset

11. megam_preprocess.py:
    -Preprocesses data to be compatible with MegaM
    -USAGE: python3 megam_preprocess.py INPUT OUTPUT INSERT_DUMMY_LABEL[1/0]
    -Input and Output denote the corresponding files
    -INSERT DUMMY LABEL = 1 when you want the processor to insert a dummy label for generating a file used to classification in MegaM

12. megam_postprocess.py:
    -Converts output generated by MegaM to labels used in this project (POSITIVE, NEGATIVE, SPAM, HAM)
    -USAGE: python3 megam_postprocess.py INPUT OUTPUT TYPE
    -TYPE = 'imdb' for sentiment dataset
            'enron' for spam/ham dataset


################################################################################
GENERAL USAGE GUIDELINES
################################################################################

1.  Always Preprocess all data. The code uses feature identifier values starting from 1 for all classifiers. Preprocessing ensures the data is in
    the format needed by all the codes.
2.  Alwasy use the preprocessed data for any of the following tasks like svm_preprocess, de-labeling, etc. to make sure all scripts stay compatible.


################################################################################
ANALYSIS AND INFERENCES
IMDB DATA for SENTIMENT PREDICTION
################################################################################

1.  Using 75% of the data for training and 25% for testing

    -The above configuration yielded the following results in the different classifiers


        a.  Naive Bayes Classifier----------------------------------------------
            ACCURACY:               85.872%
            PRECISION(POSITIVE):    88.47457627118645%
            PRECISION(NEGATIVE):    83.54545454545455%
            RECALL(POSITIVE):       82.77830637488105%
            RECALL(NEGATIVE):       89.02163383919923%
            F1(POSITIVE):           0.855317057184991
            F1(NEGATIVE):           0.8619665468188212

        b.  SVMLight Classifier-------------------------------------------------
            ACCURACY:               86.848%
            PRECISION(POSITIVE):    86.00556070435589%
            PRECISION(NEGATIVE):    87.75307002987056%
            RECALL(POSITIVE):       88.29686013320647%
            RECALL(NEGATIVE):       85.37294155634486%
            F1(POSITIVE):           0.8713615023474178
            F1(NEGATIVE):           0.865466448445172

        c.  MegaM Classifier----------------------------------------------------
            ACCURACY:               88.304%
            PRECISION(POSITIVE):    88.69009584664536%
            PRECISION(NEGATIVE):    87.91666666666667%
            RECALL(POSITIVE):       88.04313352362828%
            RECALL(NEGATIVE):       88.56958346787214%
            F1(POSITIVE):           0.8836543052681841
            F1(NEGATIVE):           0.882419173234679


2. Using 25% of the data for training and 75% for testing

    -The above configuration yielded the following results in the different classifiers


        a.  Naive Bayes Classifier----------------------------------------------
            ACCURACY:               83.53066666666666%
            PRECISION(POSITIVE):    86.56559766763849%
            PRECISION(NEGATIVE):    80.97297297297297%
            RECALL(POSITIVE):       79.31402927663211%
            RECALL(NEGATIVE):       87.73293578958578%
            F1(POSITIVE):           0.8278130924500947
            F1(NEGATIVE):           0.8421752018808136

        b.  SVMLight Classifier-------------------------------------------------
            ACCURACY:               84.0906666667%
            PRECISION(POSITIVE):    82.2345803842%
            PRECISION(NEGATIVE):    86.1625282167%
            RECALL(POSITIVE):       86.9003098622%
            RECALL(NEGATIVE):       81.2905973805%
            F1(POSITIVE):           0.845030910697
            F1(NEGATIVE):           0.836556900992

        c.  MegaM Classifier----------------------------------------------------
            ACCURACY:               85.136%
            PRECISION(POSITIVE):    84.11544850498339%
            PRECISION(NEGATIVE):    86.21408203553412%
            RECALL(POSITIVE):       86.56907789293729%
            RECALL(NEGATIVE):       83.7078053455436%
            F1(POSITIVE):           0.8532462745511031
            F1(NEGATIVE):           0.8494246042465827
