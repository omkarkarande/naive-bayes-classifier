################################################################################
README - INSTRUCTIONS ON RUNNING EACH SCRIPT
AUTHOR: OMKAR KARANDE
USC-EMAIL: karande@usc.edu
################################################################################

FORMAT OF THE FILES:
    -Each file is in the Project data format viz. LABEL FEATURE1:VALUE1 FEATURE2:VALUE2 ...
    -FEATURE identifiers start from 1 for all files.


The included scripts and their descriptions are as follows:


1.  nblearn.py:
    -This is the python script to call the Naive Bayes classifier in learning mode.
    -USAGE: python3 nblearn.py <Training File> <Model File>
    -Takes in a training file in the Project Data Format and generates a model file with the name specified.

2.  nbclassify.py:
    -This is the python script to call the Naive Bayes classifier in classification/prediction mode.
    -USAGE: python3 nbclassify.py <Model File Generated by nblearn.py> <Testing File>
    -NOTE: Testing data must be in Project data format and must NOT contain any labels. Each line should only consist of features and their respective values.
    -Dumps the predictions to stdout. use '>' to write into a file.

3.  NB.py:
    -This is my python implementation of the Naive Bayes Classifier.
    -Generic implementation that is used by both nblearn.py and nbclassify.py
    -IMPORTANT: REQUIRED TO RUN nblearn and nbclassify

4.  data_partitioner.py:
    -This python script randomizes the data in the file passed to it as an argument and generate splits based on the percentage provided
    -Usage: python3 data_partitioner.py <File> <Trainingset percentage> <Linear split or Random>
            File: Path to the file you wish to split
            Percentage: integer value between 1 to 100 - Determines the size of the Training Set
            Linear/Random: 1 for linear split and 0 for random split

5.  analysis.py:
    -Calculates the statistics of the prediction result obtained.
    -USAGE: python3 analysis.py <Actual Labels> <Predicted Labels>
    -NOTE: Files should contain labels in their text forms either POSITIVE, NEGATIVE, SPAM or HAM. Each line should contain the prediction for the record at that line.

6.  preprocess_delabel.py:
    -Removes labels from labeled files in PDF format
    -USAGE: python3 preprocess_delabel.py <input file> <output file>

7.  preprocess_imdb.py:
    -Preprocesses the IMDB data
    -Converts numeric labels >=7 to 'POSITIVE' and <=4 to 'NEGATIVE'
    -Increments the feature identifier by 1 for all features in the given file
    -USAGE:  python3 preprocess_imdb.py INPUT OUTPUT LABELED/UNLABELED[1/0]
            Labeled/Unlabeled   = 1 for files containing labels
                                = 0 for files not containing labels (feature ID increment only)

8.  preprocess_email.py:
    -Preprocesses the Email dataset
    -Reads emails from all files from the given directory and compiles them into a single dataset
    -Orders the file according to filename for Testing Data only
    -USAGE: python3 preprocess_email.py INPUT OUTPUT VOCAB LABELED/UNLABELED[1/0]
            INPUT - Directory with all the emails in
            OUTPUT - File name for the compiled corpus
            VOCAB - Path to the vocabulary file
            LABELED/UNLABELED   = 1 for LABELED DATA like the Training data where order is not important and labels exist
                                = 0 for UN-Labeled data like the Testing data where order is important and labels dont exist

9.  svm_preprocess.py:
    -Preprocesses data to be compatible with SVMlight
    -USAGE: python3 svm_preprocess.py INPUT OUTPUT INSERT_DUMMY_LABEL[1/0]
    -Input and Output denote the corresponding files
    -INSERT DUMMY LABEL = 1 when you want the processor to insert a dummy label for generating a file used to classification in SVMLight

10. svm_postprocess.py:
    -Converts output generated by SVMLight to labels used in this project (POSITIVE, NEGATIVE, SPAM, HAM)
    -USAGE: python3 svm_postprocess.py INPUT OUTPUT TYPE
    -TYPE = 'imdb' for sentiment dataset
            'enron' for spam/ham dataset

11. megam_preprocess.py:
    -Preprocesses data to be compatible with MegaM
    -USAGE: python3 megam_preprocess.py INPUT OUTPUT INSERT_DUMMY_LABEL[1/0]
    -Input and Output denote the corresponding files
    -INSERT DUMMY LABEL = 1 when you want the processor to insert a dummy label for generating a file used to classification in MegaM

12. megam_postprocess.py:
    -Converts output generated by MegaM to labels used in this project (POSITIVE, NEGATIVE, SPAM, HAM)
    -USAGE: python3 megam_postprocess.py INPUT OUTPUT TYPE
    -TYPE = 'imdb' for sentiment dataset
            'enron' for spam/ham dataset


################################################################################
GENERAL USAGE GUIDELINES
################################################################################

1.  Always Preprocess all data. The code uses feature identifier values starting from 1 for all classifiers. Preprocessing ensures the data is in
    the format needed by all the codes.
2.  Alwasy use the preprocessed data for any of the following tasks like svm_preprocess, de-labeling, etc. to make sure all scripts stay compatible.


################################################################################
ANALYSIS AND INFERENCES
IMDB DATA for SENTIMENT PREDICTION
################################################################################

1.  Using 75% of the data for training and 25% for testing

    -The above configuration yielded the following results in the different classifiers


        a.  Naive Bayes Classifier----------------------------------------------
            ACCURACY:               85.872%
            PRECISION(POSITIVE):    88.47457627118645%
            PRECISION(NEGATIVE):    83.54545454545455%
            RECALL(POSITIVE):       82.77830637488105%
            RECALL(NEGATIVE):       89.02163383919923%
            F1(POSITIVE):           0.855317057184991
            F1(NEGATIVE):           0.8619665468188212

        b.  SVMLight Classifier-------------------------------------------------
            ACCURACY:               86.848%
            PRECISION(POSITIVE):    86.00556070435589%
            PRECISION(NEGATIVE):    87.75307002987056%
            RECALL(POSITIVE):       88.29686013320647%
            RECALL(NEGATIVE):       85.37294155634486%
            F1(POSITIVE):           0.8713615023474178
            F1(NEGATIVE):           0.865466448445172

        c.  MegaM Classifier----------------------------------------------------
            ACCURACY:               88.304%
            PRECISION(POSITIVE):    88.69009584664536%
            PRECISION(NEGATIVE):    87.91666666666667%
            RECALL(POSITIVE):       88.04313352362828%
            RECALL(NEGATIVE):       88.56958346787214%
            F1(POSITIVE):           0.8836543052681841
            F1(NEGATIVE):           0.882419173234679


2. Using 25% of the data for training and 75% for testing

    -The above configuration yielded the following results in the different classifiers


        a.  Naive Bayes Classifier----------------------------------------------
            ACCURACY:               83.53066666666666%
            PRECISION(POSITIVE):    86.56559766763849%
            PRECISION(NEGATIVE):    80.97297297297297%
            RECALL(POSITIVE):       79.31402927663211%
            RECALL(NEGATIVE):       87.73293578958578%
            F1(POSITIVE):           0.8278130924500947
            F1(NEGATIVE):           0.8421752018808136

        b.  SVMLight Classifier-------------------------------------------------
            ACCURACY:               84.0906666667%
            PRECISION(POSITIVE):    82.2345803842%
            PRECISION(NEGATIVE):    86.1625282167%
            RECALL(POSITIVE):       86.9003098622%
            RECALL(NEGATIVE):       81.2905973805%
            F1(POSITIVE):           0.845030910697
            F1(NEGATIVE):           0.836556900992

        c.  MegaM Classifier----------------------------------------------------
            ACCURACY:               85.136%
            PRECISION(POSITIVE):    84.11544850498339%
            PRECISION(NEGATIVE):    86.21408203553412%
            RECALL(POSITIVE):       86.56907789293729%
            RECALL(NEGATIVE):       83.7078053455436%
            F1(POSITIVE):           0.8532462745511031
            F1(NEGATIVE):           0.8494246042465827


From the above results it is evident that both Naive Bayes and SVMLight took a hit of about 2-3% when the training set was reduced to a third
of what was used before. Almost all the different parameters of the two classifiers got reduced by only about 2% on a huge drop in the data
available for training.
MegaM - Maximum Entropy modeling is clearly the one with the highest accuracy in the list. As Maximum entropy model finds connections
between tokens unlike Naive Bayes which merely grabs the token probabilities, Maximum Entropy models perform better than naive bayes when the
data to train on is small.
As seen from the above example, in both cases MegaM surpasses Naive Bayes in almost all its statistics.

MAXIMUM ACCURACY -  MegaM

PERFORMANCE DROP -  Naive Bayes:    2.3%
                    SVMLight:       2.8%
                    MegaM:          3.2%

ROBUSTNESS - Naive Bayes and SVMLight are the most robust of them all with the lowest performance drops


################################################################################
ANALYSIS AND INFERENCES
ENRON DATA for SPAM/HAM DETECTION
################################################################################

1.  Using 75% of the data for training and 25% for testing

    -The above configuration yielded the following results in the different classifiers


        a.  Naive Bayes Classifier----------------------------------------------
            ACCURACY:               98.50477391460998%
            PRECISION(SPAM):        97.97657082002131%
            PRECISION(HAM):         99.04901243599123%
            RECALL(SPAM):           99.06676238334529%
            RECALL(HAM):            97.9385171790235%
            F1(SPAM):               0.9851865072282706
            F1(HAM):                0.9849063466084743

        b.  SVMLight Classifier-------------------------------------------------
            ACCURACY:               95.51432174382994%
            PRECISION(SPAM):        92.24109224109223%
            PRECISION(HAM):         99.37205651491365%
            RECALL(SPAM):           99.4256999282125%
            RECALL(HAM):            91.5732368896926%
            F1(SPAM):               0.9569873898773537
            F1(HAM):                0.9531338226990401

        c.  MegaM Classifier----------------------------------------------------
            ACCURACY:               98.10844892812106%
            PRECISION(SPAM):        96.95271453590193%
            PRECISION(HAM):         99.33234421364985%
            RECALL(SPAM):           99.35391241923905%
            RECALL(HAM):            96.85352622061482%
            F1(SPAM):               0.981386279028541
            F1(HAM):                0.9807727522431788


2. Using 25% of the data for training and 75% for testing

    -The above configuration yielded the following results in the different classifiers


        a.  Naive Bayes Classifier----------------------------------------------
            ACCURACY:               98.1504833964%
            PRECISION(SPAM):        97.8939810937%
            PRECISION(HAM):         98.4088717454%
            RECALL(SPAM):           98.4121255864%
            RECALL(HAM):            97.8896882494%
            F1(SPAM):               0.981523695261
            F1(HAM):                0.981485934119

        b.  SVMLight Classifier-------------------------------------------------
            ACCURACY:               92.84213054704858%
            PRECISION(SPAM):        88.01922050186866%
            PRECISION(HAM):         99.03951701427003%
            RECALL(SPAM):           99.15794538674365%
            RECALL(HAM):            86.54676258992806%
            F1(SPAM):               0.9325715578685371
            F1(HAM):                0.923726644484259

        c.  MegaM Classifier----------------------------------------------------
            ACCURACY:               97.62205008106648%
            PRECISION(SPAM):        96.20637329286798%
            PRECISION(HAM):         99.1219391540935%
            RECALL(SPAM):           99.14591603512571%
            RECALL(HAM):            96.1031175059952%
            F1(SPAM):               0.9765402843601896
            F1(HAM):                0.9758918787288446

In contrast to the IMDB data, where SVM performed better than Naive Bayes, we can see here that Naive Bayes is more accurate than SVM.
This is mainly due to the availability of a richer feature set in the Enron corpus as compared to the IMDB corpus.
Although the dataset was small in size, each document in this dataset had more tokens as compared to the documents in the sentiment corpus.

The performance drop in the accuracy of Naive Bayes when trained on 25% data rather than 75% is negligible, a mere 0.35%. Whereas, SVM saw a significant drop.
The performance of MegaM is comparable to Naive Bayes.

MAXIMUM ACCURACY -  NAive Bayes

PERFORMANCE DROP -  Naive Bayes:    0.35%
                    SVMLight:       2.7%
                    MegaM:          0.48%

ROBUSTNESS - Naive Bayes and MegaM are the most robust of them all with the least performance drops.
